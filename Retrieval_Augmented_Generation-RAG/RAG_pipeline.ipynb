{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain chromadb sentence-transformers transformers torch accelerate"
      ],
      "metadata": {
        "id": "IvU-RxWAJeOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries first\n",
        "# pip install langchain chromadb sentence-transformers transformers torch accelerate\n",
        "\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import torch\n",
        "import os"
      ],
      "metadata": {
        "id": "d2N1xG1DJnz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Initialize Embeddings\n",
        "embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
        "\n",
        "# Step 2: Initialize ChromaDB (Vector Database)\n",
        "persist_directory = \"./chroma_rag_db\"\n",
        "if not os.path.exists(persist_directory):\n",
        "    os.makedirs(persist_directory)"
      ],
      "metadata": {
        "id": "bf1xv2AsJ-Qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Add Documents (Knowledge Base)\n",
        "texts = [\n",
        "    \"The capital of France is Paris.\",\n",
        "    \"The Great Wall of China is visible from space.\",\n",
        "    \"Mistral 7B is a powerful open-source language model.\",\n",
        "    \"LangChain is a framework for building LLM-powered applications.\"\n",
        "]\n",
        "metadatas = [{\"source\": f\"doc_{i}\"} for i in range(len(texts))]\n",
        "\n",
        "db = Chroma.from_texts(texts, embeddings, metadatas=metadatas, persist_directory=persist_directory)\n",
        "db.persist()"
      ],
      "metadata": {
        "id": "G6Gmw7XJKA-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkWIDfOsJZRg"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Step 4: Load the Language Model\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"  # Or any Huggingface instruct model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\", trust_remote_code=True)\n",
        "\n",
        "llm_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=300,\n",
        "    temperature=0.3,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=llm_pipeline)\n",
        "\n",
        "# Step 5: Build the RAG Pipeline (Retriever + LLM)\n",
        "rag_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=db.as_retriever(search_kwargs={\"k\": 2}),\n",
        "    chain_type=\"stuff\"  # Stuff retrieved docs into context\n",
        ")\n",
        "\n",
        "# Step 6: Ask a Question!\n",
        "query = \"What is LangChain and why is it useful?\"\n",
        "result = rag_chain.run(query)\n",
        "\n",
        "print(\"\\n=== Final Answer ===\\n\")\n",
        "print(result)\n",
        "\n",
        "# Step 7: Cleanup\n",
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mvvHjXMXkNyn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PtBnzT9wkN93"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g4yiLi0okOIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WFDQyaVxkH1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PMwgWekvh0Pl"
      }
    }
  ]
}