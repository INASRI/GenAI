{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "id": "b2wiqwEYj6c6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-openai\n",
        "!pip install langchain_community"
      ],
      "metadata": {
        "id": "aZIfrAZ3j9IB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "id": "wKZZjEx8j-Ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb"
      ],
      "metadata": {
        "id": "gVi4dzcKkDQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "id": "fntAveI6ktx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF"
      ],
      "metadata": {
        "id": "eoglPNWFltbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HwhgUdS8mCGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import initialize_agent, AgentType, Tool\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "import os"
      ],
      "metadata": {
        "id": "H9kZKaXhk2TJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- 1. Set OpenAI Key ----\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"  # Replace with your key"
      ],
      "metadata": {
        "id": "fVVGydJbk6Rx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "id": "8nHv9_RHm0TK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import Tool, initialize_agent, AgentType\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.tools import tool\n",
        "from PyPDF2 import PdfReader\n",
        "import os"
      ],
      "metadata": {
        "id": "REzafmQdm6Th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntVX_oYajrMB"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Step 1: Tool to extract text from PDF\n",
        "@tool\n",
        "def extract_pdf_text(file_path: str) -> str:\n",
        "    \"\"\"Extract all text from a PDF file.\"\"\"\n",
        "    try:\n",
        "        reader = PdfReader(file_path)\n",
        "        full_text = \"\"\n",
        "        for page in reader.pages:\n",
        "            full_text += page.extract_text()\n",
        "        return full_text[:5000]  # Truncate for summarization if too long\n",
        "    except Exception as e:\n",
        "        return f\"Error reading PDF: {e}\"\n",
        "\n",
        "# Step 2: Tool to summarize text\n",
        "@tool\n",
        "def summarize_text(text: str) -> str:\n",
        "    \"\"\"Summarize the given input text using an LLM.\"\"\"\n",
        "    llm = ChatOpenAI(temperature=0.2, model_name=\"gpt-3.5-turbo\")\n",
        "    prompt = f\"Please summarize the following PDF content in simple terms:\\n\\n{text}\"\n",
        "    return llm.predict(prompt)\n",
        "\n",
        "# Step 3: Register tools\n",
        "tools = [extract_pdf_text, summarize_text]\n",
        "\n",
        "# Step 4: Initialize Agent\n",
        "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
        "\n",
        "agent = initialize_agent(\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Run agent on your PDF file\n",
        "def run_pdf_agent(pdf_path: str):\n",
        "    print(\"Running agent on PDF:\", pdf_path)\n",
        "    query = f\"Extract text from the PDF at path '{pdf_path}' and then summarize it.\" # More specific instructions\n",
        "    result = agent.run(query) # Using agent.run instead of agent.invoke to handle parsing errors automatically\n",
        "    print(\"\\nFinal Summary:\\n\", result)\n",
        "\n",
        "# Replace with your actual uploaded file path\n",
        "run_pdf_agent(\"/content/sample_data/Robotics-paper-one.pdf\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyikp_EZnJpZ",
        "outputId": "e9444a54-a8ee-4c94-8f80-cf055282a5e4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running agent on PDF: /content/sample_data/Robotics-paper-one.pdf\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mQuestion: Extract text from the PDF at path '/content/sample_data/Robotics-paper-one.pdf' and then summarize it.\n",
            "Thought: I should first extract the text from the PDF file and then summarize it to get a concise overview.\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"extract_pdf_text\",\n",
            "  \"action_input\": \"/content/sample_data/Robotics-paper-one.pdf\"\n",
            "}\n",
            "```\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:PyPDF2.generic._base:FloatObject (b'0.00-40') invalid; use 0.0 instead\n",
            "WARNING:PyPDF2.generic._base:FloatObject (b'0.00-40') invalid; use 0.0 instead\n",
            "WARNING:PyPDF2.generic._base:FloatObject (b'0.00-40') invalid; use 0.0 instead\n",
            "WARNING:PyPDF2.generic._base:FloatObject (b'0.00-40') invalid; use 0.0 instead\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Observation: \u001b[36;1m\u001b[1;3mPreprint\n",
            "RT-1: R OBOTICS TRANSFORMER\n",
            "FOR REAL-WORLD CONTROL AT SCALE\n",
            "1Anthony Brohan∗, Noah Brown∗, Justice Carbajal∗, Yevgen Chebotar∗, Joseph Dabis∗,\n",
            "Chelsea Finn∗, Keerthana Gopalakrishnan∗, Karol Hausman∗, Alex Herzog†, Jasmine Hsu∗,\n",
            "Julian Ibarz∗, Brian Ichter∗, Alex Irpan∗, Tomas Jackson∗, Sally Jesmonth∗, Nikhil J Joshi∗,\n",
            "Ryan Julian∗, Dmitry Kalashnikov∗, Yuheng Kuang∗, Isabel Leal∗, Kuang-Huei Lee‡, Sergey Levine∗,\n",
            "Yao Lu∗, Utsav Malla∗, Deeksha Manjunath∗, Igor Mordatch‡, Ofir Nachum‡, Carolina Parada∗,\n",
            "Jodilyn Peralta∗, Emily Perez∗, Karl Pertsch∗, Jornell Quiambao∗, Kanishka Rao∗, Michael Ryoo∗,\n",
            "Grecia Salazar∗, Pannag Sanketi∗, Kevin Sayed∗, Jaspiar Singh∗, Sumedh Sontakke‡, Austin Stone∗,\n",
            "Clayton Tan∗, Huong Tran∗, Vincent Vanhoucke∗, Steve Vega∗, Quan Vuong∗, Fei Xia∗, Ted Xiao∗,\n",
            "Peng Xu∗, Sichun Xu∗, Tianhe Yu∗, Brianna Zitkovich∗\n",
            "∗Robotics at Google,†Everyday Robots,‡Google Research, Brain Team\n",
            "ABSTRACT\n",
            "By transferring knowledge from large, diverse, task-agnostic datasets, modern ma-\n",
            "chine learning models can solve specific downstream tasks either zero-shot or with\n",
            "small task-specific datasets to a high level of performance. While this capability\n",
            "has been demonstrated in other fields such as computer vision, natural language\n",
            "processing or speech recognition, it remains to be shown in robotics, where the\n",
            "generalization capabilities of the models are particularly critical due to the dif-\n",
            "ficulty of collecting real-world robotic data. We argue that one of the keys to\n",
            "the success of such general robotic models lies with open-ended task-agnostic\n",
            "training, combined with high-capacity architectures that can absorb all of the di-\n",
            "verse, robotic data. In this paper, we present a model class, dubbed Robotics\n",
            "Transformer, that exhibits promising scalable model properties. We verify our\n",
            "conclusions in a study of different model classes and their ability to generalize as\n",
            "a function of the data size, model size, and data diversity based on a large-scale\n",
            "data collection on real robots performing real-world tasks. The project’s website\n",
            "and videos can be found at robotics-transformer1.github.io\n",
            "1 I NTRODUCTION\n",
            "End-to-end robotic learning, with either imitation or reinforcement, typically involves collecting\n",
            "task-specific data in either single-task (Kalashnikov et al., 2018; Zhang et al., 2018) or multi-\n",
            "task (Kalashnikov et al., 2021b; Jang et al., 2021) settings that are narrowly tailored to the tasks\n",
            "that the robot should perform. This workflow mirrors the classic approach to supervised learning in\n",
            "other domains, such as computer vision and NLP, where task-specific datasets would be collected,\n",
            "labeled, and deployed to solve individual tasks, with little interplay between the tasks themselves.\n",
            "Recent years have seen a transformation in vision, NLP, and other domains, away from siloed, small-\n",
            "scale datasets and models and towards large, general models pre-trained on broad, large datasets.\n",
            "The keys to the success of such models lie with open-ended task-agnostic training, combined with\n",
            "high-capacity architectures that can absorb all of the knowledge present in large-scale datasets. If a\n",
            "model can “sponge up” experience to learn general patterns in language or perception, then it can\n",
            "bring them to bear on individual tasks more efficiently. While removing the need for large task-\n",
            "specific datasets is appealing generally in supervised learning, it is even more critical in robotics,\n",
            "where datasets might require engineering-heavy autonomous operation or expensive human demon-\n",
            "strations. We therefore ask: can we train a single, capable, large multi-task backbone model on data\n",
            "consisting of a wide variety of robotic tasks? And does such a model enjoy the benefits observed in\n",
            "other domains, exhibiting zero-shot generalization to new tasks, environments, and objects?\n",
            "Building such models in robotics is not easy. Although recent years have seen several large multi-\n",
            "task robot policies proposed in the literature (Reed et al., 2022; Jang et al., 2021), such models often\n",
            "have limited breadth of real-world tasks, as with Gato (Reed et al., 2022), or focus on training tasks\n",
            "rather than generalization to new tasks, as with recent instruction following methods (Shridhar et al.,\n",
            "2021; 2022), or attain comparatively lower performance on new tasks (Jang et al., 2021).\n",
            "1Authors listed in alphabetical order. Contributions in Appendix A.\n",
            "Corresponding emails: {keerthanapg,kanishkarao,karolhausman }@google.com .\n",
            "1arXiv:2212.06817v2  [cs.RO]  11 Aug 2023Preprint\n",
            "  Mode                     Arm                        Base Pick apple from top drawer and place on counter Instruction \n",
            "Images \n",
            "FiLM \n",
            "EﬃcientNet \n",
            "Transformer \n",
            "TokenLearner\n",
            "… R T -1 \n",
            "3 Hz\n",
            "(1+γ)               β\n",
            "·\n",
            "+Action \n",
            "  Mode                     Arm                        Base Pick apple from top drawer and place on counter Instruction \n",
            "Images \n",
            "FiLM \n",
            "EﬃcientNet \n",
            "Transformer \n",
            "TokenLearner… R T -1 \n",
            "3 Hz(1+γ)               β\n",
            "·\n",
            "+Action \n",
            "(a) RT-1 takes images and natural language\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m```\n",
            "{\n",
            "  \"action\": \"summarize_text\",\n",
            "  \"action_input\": \"Preprint RT-1: R OBOTICS TRANSFORMER FOR REAL-WORLD CONTROL AT SCALE 1Anthony Brohan∗, Noah Brown∗, Justice Carbajal∗, Yevgen Chebotar∗, Joseph Dabis∗, Chelsea Finn∗, Keerthana Gopalakrishnan∗, Karol Hausman∗, Alex Herzog†, Jasmine Hsu∗, Julian Ibarz∗, Brian Ichter∗, Alex Irpan∗, Tomas Jackson∗, Sally Jesmonth∗, Nikhil J Joshi∗, Ryan Julian∗, Dmitry Kalashnikov∗, Yuheng Kuang∗, Isabel Leal∗, Kuang-Huei Lee‡, Sergey Levine∗, Yao Lu∗, Utsav Malla∗, Deeksha Manjunath∗, Igor Mordatch‡, Ofir Nachum‡, Carolina Parada∗, Jodilyn Peralta∗, Emily Perez∗, Karl Pertsch∗, Jornell Quiambao∗, Kanishka Rao∗, Michael Ryoo∗, Grecia Salazar∗, Pannag Sanketi∗, Kevin Sayed∗, Jaspiar Singh∗, Sumedh Sontakke‡, Austin Stone∗, Clayton Tan∗, Huong Tran∗, Vincent Vanhoucke∗, Steve Vega∗, Quan Vuong∗, Fei Xia∗, Ted Xiao∗, Peng Xu∗, Sichun Xu∗, Tianhe Yu∗, Brianna Zitkovich∗ ∗Robotics at Google,†Everyday Robots,‡Google Research, Brain Team ABSTRACT By transferring knowledge from large, diverse, task-agnostic datasets, modern machine learning models can solve specific downstream tasks either zero-shot or with small task-specific datasets to a high level of performance. While this capability has been demonstrated in other fields such as computer vision, natural language processing or speech recognition, it remains to be shown in robotics, where the generalization capabilities of the models are particularly critical due to the difficulty of collecting real-world robotic data. We argue that one of the keys to the success of such general robotic models lies with open-ended task-agnostic training, combined with high-capacity architectures that can absorb all of the diverse, robotic data. In this paper, we present a model class, dubbed Robotics Transformer, that exhibits promising scalable model properties. We verify our conclusions in a study of different model classes and their ability to generalize as a function of the data size, model size, and data diversity based on a large-scale data collection on real robots performing real-world tasks. The project’s website and videos can be found at robotics-transformer1.github.io 1 I NTRODUCTION End-to-end robotic learning, with either imitation or reinforcement, typically involves collecting task-specific data in either single-task (Kalashnikov et al., 2018; Zhang et al., 2018) or multi-task (Kalashnikov et al., 2021b; Jang et al., 2021) settings that are narrowly tailored to the tasks that the robot should perform. This workflow mirrors the classic approach to supervised learning in other domains, such as computer vision and NLP, where task-specific datasets would be collected, labeled, and deployed to solve individual tasks, with little interplay between the tasks themselves. Recent years have seen a transformation in vision, NLP, and other domains, away from siloed, small- scale datasets and models and towards large, general models pre-trained on broad, large datasets. The keys to the success of such models lie with open-ended task-agnostic training, combined with high-capacity architectures that can absorb all of the knowledge present in large-scale datasets. If a model can “sponge up” experience to learn general patterns in language or perception, then it can bring them to bear on individual tasks more efficiently. While removing the need for large task- specific datasets is appealing generally in supervised learning, it is even more critical in robotics, where datasets might require engineering-heavy autonomous operation or expensive human demonstrations. We therefore ask: can we train a single, capable, large multi-task backbone model on data consisting of a wide variety of robotic tasks? And does such a model enjoy the benefits observed in other domains, exhibiting zero-shot generalization to new tasks, environments, and objects? Building such models in robotics is not easy. Although recent years have seen several large multi- task robot policies proposed in the literature (Reed et al., 2022; Jang et al., 2021), such models often have limited breadth of real-world tasks, as with Gato (Reed et al., 2022), or focus on training tasks rather than generalization to new tasks, as with recent instruction following methods (Shridhar et al., 2021; 2022), or attain comparatively lower performance on new tasks (Jang et al., 2021). 1Authors listed in alphabetical order. Contributions in Appendix A. Corresponding emails: {keerthanapg,kanishkarao,karolhausman }@google.com . 1arXiv:2212.06817v2 [cs.RO] 11 Aug 2023Preprint Mode Arm Base Pick apple from top drawer and place on counter Instruction Images FiLM EﬃcientNet Transformer TokenLearner… R T -1 3 Hz(1+γ) β · +Action Mode Arm Base Pick apple from top drawer and place on counter Instruction Images FiLM EﬃcientNet Transformer TokenLearner… R T -1 3 Hz(1+γ) β · +Action (a) RT-1 takes images and natural language\"\n",
            "}\n",
            "```\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mThe PDF discusses the development of a Robotics Transformer model that can learn from diverse datasets and perform various tasks without needing specific task-related data. This model aims to generalize well to new tasks, environments, and objects in the field of robotics. The paper explores the benefits of training a single, large multi-task model on a wide range of robotic tasks and investigates its ability to perform new tasks without the need for task-specific data. The model is compared to existing robot policies and instruction following methods, showing promising results in terms of performance and generalization.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mFinal Answer: The PDF discusses the development of a Robotics Transformer model that aims to generalize well to new tasks, environments, and objects in the field of robotics by learning from diverse datasets without needing specific task-related data.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "Final Summary:\n",
            " The PDF discusses the development of a Robotics Transformer model that aims to generalize well to new tasks, environments, and objects in the field of robotics by learning from diverse datasets without needing specific task-related data.\n"
          ]
        }
      ]
    }
  ]
}