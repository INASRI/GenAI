{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AH6gi3rK6yDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What This Script Does:\n",
        "# Builds a LLaMA-style model\n",
        "# Adds LoRA adapters only to attention projection layers\n",
        "# Trains only those adapters\n",
        "# Saves a lightweight, tuned model"
      ],
      "metadata": {
        "id": "UV1sRxow6yv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üì¶ Install required libraries\n",
        "!pip install -q transformers datasets accelerate peft bitsandbytes"
      ],
      "metadata": {
        "id": "CzCmzfyPdwKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üß† Imports\n",
        "from transformers import AutoTokenizer, LlamaConfig, LlamaForCausalLM, TrainingArguments, Trainer\n",
        "from datasets import Dataset\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from transformers import DataCollatorForLanguageModeling"
      ],
      "metadata": {
        "id": "QB6kubKNdyqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "Mm5Eipx2eBld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")"
      ],
      "metadata": {
        "id": "d6FjSlx5eD3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üõ† Define a lightweight LLaMA-style model config\n",
        "config = LlamaConfig(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    hidden_size=512,\n",
        "    intermediate_size=2048,\n",
        "    num_attention_heads=8,\n",
        "    num_hidden_layers=4,\n",
        "    max_position_embeddings=512,\n",
        "    bos_token_id=tokenizer.bos_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.pad_token_id\n",
        ")\n",
        "\n",
        "# üîß Build the model\n",
        "model = LlamaForCausalLM(config)"
      ],
      "metadata": {
        "id": "J3xov7lriy6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üîÅ Apply LoRA with PEFT\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # attention layers\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()  # Only LoRA params will be trained\n",
        "\n",
        "# üìò Toy dataset\n",
        "texts = [\n",
        "    \"AI is transforming healthcare and education.\",\n",
        "    \"Yoga therapy helps relieve back pain and anxiety.\",\n",
        "    \"Robots use sensors to navigate environments.\"\n",
        "]\n",
        "dataset = Dataset.from_dict({\"text\": texts})"
      ],
      "metadata": {
        "id": "JWw2kLCKi8TO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Tokenize Dataset\n",
        "def tokenize(example):\n",
        "    # Set the padding token to the EOS token if it's not already set\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize)"
      ],
      "metadata": {
        "id": "SFmQfOXQjR5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Data Collator\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# 6. Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./llama-pretrain-demo\",\n",
        "    per_device_train_batch_size=2,\n",
        "    num_train_epochs=3,\n",
        "    logging_steps=5,\n",
        "    save_steps=10,\n",
        "    save_total_limit=1,\n",
        "    report_to=\"none\"\n",
        ")"
      ],
      "metadata": {
        "id": "EHjxvOgv6ZLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTrW203Ddsj0"
      },
      "outputs": [],
      "source": [
        "# üöÄ Train with LoRA\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# üíæ Save model\n",
        "model.save_pretrained(\"./lora-llama-demo\")\n",
        "tokenizer.save_pretrained(\"./lora-llama-demo\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from peft import PeftModel, PeftConfig"
      ],
      "metadata": {
        "id": "kAdDX_dz7d8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üîÑ Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./lora-llama-demo\")\n",
        "\n",
        "# üß† Load base model config\n",
        "from transformers import LlamaConfig, LlamaForCausalLM"
      ],
      "metadata": {
        "id": "OXcAS85X7g1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This should match your original model's config\n",
        "config = LlamaConfig(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    hidden_size=512,\n",
        "    intermediate_size=2048,\n",
        "    num_attention_heads=8,\n",
        "    num_hidden_layers=4,\n",
        "    max_position_embeddings=512,\n",
        "    bos_token_id=tokenizer.bos_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.pad_token_id\n",
        ")\n",
        "\n",
        "# üîß Load base model and inject trained LoRA weights\n",
        "base_model = LlamaForCausalLM(config)\n",
        "model = PeftModel.from_pretrained(base_model, \"./lora-llama-demo\")\n",
        "model.eval()  # set to inference mode"
      ],
      "metadata": {
        "id": "Sjtn2WCY7kkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üßæ Define a prompt\n",
        "prompt = \"Yoga is helpful for managing stress and\"\n",
        "\n",
        "# üî¢ Tokenize input\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Import torch\n",
        "import torch\n",
        "\n",
        "# üîÆ Generate prediction\n",
        "with torch.no_grad():\n",
        "    output = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=30,\n",
        "        do_sample=True,\n",
        "        temperature=0.8,\n",
        "        top_k=50,\n",
        "        top_p=0.95\n",
        "    )\n",
        "\n",
        "# üì¢ Decode and print\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "PQdaKYMS7yy-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}